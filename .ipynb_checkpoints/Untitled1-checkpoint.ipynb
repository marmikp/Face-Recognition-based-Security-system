{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import struct\n",
    "import json\n",
    "import socket\n",
    "import h5py\n",
    "import face1 as face1\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from sklearn.manifold import TSNE\n",
    "import threading\n",
    "import os\n",
    "import time\n",
    "import six.moves.urllib as urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import cv2\n",
    "from handle import *\n",
    "sys.path.append(\"..\")\n",
    "import warnings\n",
    "# Suppress LabelEncoder warning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "from db_conn import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "print(\"Do You Want Train Data? (Y/N) : \")\n",
    "ans_user = input()\n",
    "if ans_user==\"y\" or ans_user == \"Y\":\n",
    "    face1.create_embedded()\n",
    "    print(\"Data are Trained...\")\n",
    "\n",
    "\n",
    "global_lock = threading.Lock()\n",
    "knn1 = KNeighborsClassifier(n_neighbors=2, metric='euclidean')\n",
    "\n",
    "svc1 = svm.SVC(kernel='rbf', gamma=1, C=1.0, probability=True)\n",
    "encoder1 =None\n",
    "mic = {}\n",
    "\n",
    "thresh = 0.25\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def pre_load():\n",
    "    global knn1, svc1,encoder1,mic\n",
    "    metadata = face1.load_metadata('images')\n",
    "    num = len(metadata)\n",
    "    filename = 'tempFiles/embedded2.hdf5'\n",
    "    f = h5py.File(filename, 'r')\n",
    "    \n",
    "    for raw in f:\n",
    "        for j in f[raw]:\n",
    "            xc = j.split('.')\n",
    "            mic[str(raw),int(xc[0])] = f[raw][j][:]    \n",
    "    embedded2 = np.zeros((metadata.shape[0], 128))\n",
    "    for i, m in enumerate(metadata):\n",
    "        embedded2[i] = f[m.name+\"/\"+m.file].value\n",
    "\n",
    "    \n",
    "    f.close() \n",
    "\n",
    "\n",
    "    train_in =np.ones((metadata.shape[0]), dtype=bool)\n",
    "    test_in =np.ones((metadata.shape[0]), dtype=bool)\n",
    "    name=None\n",
    "    for i,m in enumerate(metadata):    # 90 / 10 dataset \n",
    "        if name==None:\n",
    "            name=m.name\n",
    "            train_in[i]=False\n",
    "            test_in[i]=True\n",
    "            continue\n",
    "        if name==m.name:\n",
    "            train_in[i]=True\n",
    "            test_in[i]=False\n",
    "        else:\n",
    "            name=m.name\n",
    "            train_in[i]=False\n",
    "            test_in[i]=True\n",
    "    distances = [] # squared L2 distance between pairs\n",
    "    identical = [] # 1 if same identity, 0 otherwise\n",
    "\n",
    "    num = len(metadata)\n",
    "\n",
    "    for i in range(num - 1):\n",
    "        for j in range(1, num):\n",
    "            distances.append(face1.distance(embedded2[i], embedded2[j]))\n",
    "            identical.append(1 if metadata[i].name == metadata[j].name else 0)\n",
    "            \n",
    "    distances = np.array(distances)\n",
    "    identical = np.array(identical)\n",
    "\n",
    "    thresholds = np.arange(0.2, 0.4, 0.005)\n",
    "\n",
    "    #print(thresholds)\n",
    "    # for t in thresholds:\n",
    "    #     print(identical)\n",
    "    #     print(f1_score(identical, distances < t))\n",
    "    f1_scores = [f1_score(identical, distances < t) for t in thresholds]\n",
    "    acc_scores = [accuracy_score(identical, distances < t) for t in thresholds]\n",
    "\n",
    "    opt_idx = np.argmax(f1_scores)\n",
    "    # Threshold at maximal F1 score\n",
    "    opt_tau = thresholds[opt_idx]\n",
    "    # Accuracy at maximal F1 score\n",
    "    opt_acc = accuracy_score(identical, distances < opt_tau)\n",
    "    global thresh\n",
    "    thresh = opt_tau\n",
    "    print(f'Accuracy at threshold {opt_tau:.3f} = {opt_acc:.3f}')\n",
    "\n",
    "\n",
    "    #####################################\n",
    "\n",
    "\n",
    "    targets2 = np.array([m.name for m in metadata])\n",
    "\n",
    "    encoder1 = LabelEncoder()\n",
    "    encoder1.fit(targets2)\n",
    "\n",
    "    y = encoder1.transform(targets2)\n",
    "\n",
    "\n",
    "    # 50 train examples of 10 identities (5 examples each)\n",
    "    X_train = embedded2[train_in]\n",
    "    # 50 test examples of 10 identities (5 examples each)\n",
    "    X_test = embedded2[test_in]\n",
    "\n",
    "    y_train = y[train_in]\n",
    "    y_test = y[test_in]\n",
    "\n",
    "\n",
    "\n",
    "    knn1.fit(X_train, y_train)\n",
    "    svc1.fit(X_train, y_train)\n",
    "\n",
    "    acc_knn1 = accuracy_score(y_test, knn1.predict(X_test))\n",
    "    acc_svc1 = accuracy_score(y_test, svc1.predict(X_test))\n",
    "    print(f'KNN accuracy = {acc_knn1}, SVM accuracy = {acc_svc1}')\n",
    "    \n",
    "pre_load()\n",
    "##############\n",
    "\n",
    "past_time = None\n",
    "def retrain():\n",
    "        now = datetime.datetime.now()\n",
    "        global past_time\n",
    "        if past_time == None:\n",
    "                past_time = now\n",
    "        time_diff =int((now-past_time).total_seconds())/60\n",
    "        import platform\n",
    "        if platform.system() == 'Windows':\n",
    "            win =1\n",
    "        else:\n",
    "            win =0\n",
    "\n",
    "        if time_diff >0.3:\n",
    "                global main_c\n",
    "                past_time = now\n",
    "                while global_lock.locked():\n",
    "                    continue\n",
    "                global_lock.acquire()\n",
    "                print('New Face adding...')\n",
    "                main_c.execute(\"select * from add_person\")\n",
    "                data = main_c.fetchall()\n",
    "                main_c.execute(\"truncate table add_person\")\n",
    "                f = h5py.File('tempFiles/temp.hdf5','a')\n",
    "                g = h5py.File('tempFiles/embedded2.hdf5','a')\n",
    "                for d in data:\n",
    "                        if 1:\n",
    "                            iemd = f[d[1]].value\n",
    "                            del f[d[1]] \n",
    "                            gg = g.create_group(d[2])\n",
    "                            print(\"XXX\")\n",
    "                            if win !=1:\n",
    "                                os.system(\"mkdir images/\"+str(d[2]))\n",
    "                                os.system(\"mv temp_image/\"+str(d[1])+\" images/\"+str(d[2])+\"/1.jpg\")\n",
    "                                print(\"sadas\")\n",
    "                            else:\n",
    "                                os.system(\"md images\\\\\"+str(d[2]))\n",
    "                                os.system(\"move temp_image\\\\\"+str(d[1])+\" images\\\\\"+str(d[2])+\"\\\\1.jpg\")\n",
    "\n",
    "                            no_img=1\n",
    "                            gg.create_dataset(str(no_img)+\".jpg\", data=iemd)\n",
    "                            for key in f.keys():\n",
    "                                if face1.distance(f[key].value,iemd) < 0.25:\n",
    "                                    print(\"same face adding .......\")\n",
    "                                    main_c.execute('delete from notification where  img_name=%s',(key))\n",
    "                                    no_img=no_img+1\n",
    "                                    if win !=1:\n",
    "                                        os.system(\"mv temp_image/\"+str(key)+\" images/\"+str(d[2])+\"/\"+str(no_img)+\".jpg\")\n",
    "                                    else:\n",
    "                                        os.system(\"move temp_image\\\\\"+str(key)+\" images\\\\\"+str(d[2])+\"\\\\\"+str(no_img)+\".jpg\")\n",
    "                                    gg.create_dataset(str(no_img)+\".jpg\", data=f[key].value)\n",
    "                                    \n",
    "                                    del f[key]\n",
    "                                    \n",
    "                            while no_img<3:\n",
    "                                    print(no_img)\n",
    "                                    nn = no_img+1\n",
    "                                    if win !=1:\n",
    "                                        os.system(\"cp images/\"+str(d[2])+\"/\"+str(no_img)+\".jpg images/\"+str(d[2])+\"/\"+str(nn)+\".jpg\")\n",
    "                                    else:\n",
    "                                        os.system(\"copy images\\\\\"+str(d[2])+\"\\\\\"+str(no_img)+\".jpg images\\\\\"+str(d[2])+\"\\\\\"+str(nn)+\".jpg\")\n",
    "                                    no_img = no_img+1\n",
    "                                    gg.create_dataset(str(no_img)+\".jpg\", data=iemd)\n",
    "                            main_c.execute('insert into user_detail(name,no_img,last_update_date) values(%s,%s,%s)',(d[2],no_img,now.strftime(\"%Y-%m-%d\")))\n",
    "                            db.commit()\n",
    "                            \n",
    "                        # except Exception as e:\n",
    "                        #     print(e)\n",
    "                # main_c.execute(\"select * from for_asking where flag = 1\")\n",
    "                # data1 = main_c.fetchall()\n",
    "                # main_c.execute(\"delete from for_asking where flag=1\")\n",
    "                # for d1 in data1:\n",
    "                #         local_c.execute(\"select no_img from user_detail where name = %s\",(d1[1]))\n",
    "                #         no_img = local_c.fetchone()\n",
    "                #         no = no_img[0] + 1\n",
    "                #         sf = len(os.listdir(os.pajoin(path, d1[1])))\n",
    "                #         s =int(sf)+1\n",
    "                #         local_c.execute(\"update user_detail set no_img = %s\",(no))\n",
    "                #         os.system(\"mv temp_image/\"+str(d1[2])+\" images/\"+str(d1[1])+\"/\"+s+\".jpg\")\n",
    "                f.close()\n",
    "                g.close()\n",
    "                #global_lock.release()\n",
    "                print('### complete face adding ###')                #retrain model\n",
    "                if not len(data) == 0:\n",
    "                    pre_load()\n",
    "\n",
    "                \n",
    "                #retrain model\n",
    "                #retrain model\n",
    "                \n",
    "\n",
    "\n",
    "################\n",
    "\n",
    "\n",
    "def testing_run(frame):\n",
    "    global thresh\n",
    "    \n",
    "    if 1:\n",
    "        img= frame\n",
    "        face_locations = face_recognition.face_locations(img)\n",
    "        #print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "        if len(face_locations) ==0 :\n",
    "            print(\"noface\")\n",
    "            #data_maintain(\"noface\",img,np.array(np.random(1,128)))\n",
    "        else:    \n",
    "            for face_location in face_locations:\n",
    "\n",
    "                # Print the location of each face in this image\n",
    "                top, right, bottom, left = face_location\n",
    "\t\t\t\t# You can access the actual face itself like this:\n",
    "                face_image = img[top:bottom, left:right]\n",
    "                #cv2.rectangle(frame, (left-5, top-5), (right-5, bottom+5), (255,0,0), 2)\n",
    "                \n",
    "            \n",
    "                example_image = np.asarray(face_image)\n",
    "\n",
    "                fd = face_recognition.face_encodings(example_image)[0]\n",
    "                example_prediction1 = knn1.predict([fd])\n",
    "\n",
    "                example_identity1 = encoder1.inverse_transform(example_prediction1)[0]\n",
    "                \n",
    "                \n",
    "                if face1.distance(mic[(example_identity1,1)],fd)<thresh and face1.distance(mic[(example_identity1,2)],fd) < thresh:\n",
    "                    print(example_identity1)\n",
    "                    data_maintain(example_identity1, face_image,fd,example_identity1)\n",
    "                   \n",
    "                # elif face1.distance(mic[(example_identity1,0)],fd) < thresh2 and face1.distance(mic[(example_identity1,1)],fd) < thresh2 :\n",
    "                #     print(example_identity1)\n",
    "                #     print(\"2\")\n",
    "                #     for_asking(example_identity1, face_image,fd)\n",
    "                else:\n",
    "                    print(\"unknown\")\n",
    "                    data_maintain(\"unknown\", face_image,fd,example_identity1)\n",
    "            \n",
    "   \n",
    "    # except Exception as e:\n",
    "    #      print(e)\n",
    "\n",
    "###################################\n",
    "\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_to_file(img_name,emd):\n",
    "        while global_lock.locked():\n",
    "                continue\n",
    "        fb = h5py.File('tempFiles/temp.hdf5','a')\n",
    "        global_lock.acquire()\n",
    "        print('#### writing ###')\n",
    "        fb.create_dataset(img_name, data=emd)\n",
    "        fb.close()\n",
    "        global_lock.release()\n",
    "\n",
    "\n",
    "\n",
    "class myThread1(threading.Thread):\n",
    "    def __init__(self, image):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.time = time\n",
    "        self.image = image\n",
    "        self.per = None\n",
    "        # self.db = PyMySQL.connect(\"localhost\",\"root\",\"\",\"main_db\" )\n",
    "        # self.db1=PyMySQL.connect(\"localhost\",\"root\",\"\",\"local_db\" )\n",
    "        # self.main_c = self.db.cursor()\n",
    "        # self.local_c=self.db1.cursor()\n",
    "        self.img_n = None\n",
    "        self.temd =None\n",
    "      \n",
    " \n",
    "        \n",
    "    def run(self):\n",
    "        self.per = detection(self.image.copy())\n",
    "        testing_run(self.image)\n",
    "        retrain()\n",
    "        # self.db.commit()\n",
    "        # self.db1.commit()\n",
    "        # self.main_c.close()\n",
    "        # self.local_c.close()\n",
    "        # self.db.close()\n",
    "        # self.db1.close()\n",
    "        if not self.img_n ==None:\n",
    "            write_to_file(self.img_n,self.temd)\n",
    "\n",
    "        \n",
    "         \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detection(img):\n",
    "    image_np = img\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    #print(type(image_np_expanded))\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "    (boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: image_np_expanded})\n",
    "    image_np,name1 = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32),\n",
    "        np.squeeze(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "    return name1\n",
    "\n",
    "\n",
    "in_upload = False\n",
    "clientsocket=None\n",
    "encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), 90]\n",
    "past2 = datetime.datetime.now()\n",
    "\n",
    "\n",
    "def go_live(frame):\n",
    "    global in_upload,clientsocket,encode_param,past2\n",
    "    # print(in_upload)\n",
    "    if in_upload:\n",
    "        now = datetime.datetime.now()\n",
    "        result, frame = cv2.imencode('.jpg', frame, encode_param)\n",
    "        data = pickle.dumps(frame, 0)\n",
    "        size = len(data)\n",
    "        clientsocket.sendall(struct.pack(\">L\", size) + data)\n",
    "        # print(\"upload \")\n",
    "        if ((now-past2).total_seconds() / 60.0) > 1:\n",
    "            print(\"2min #########\")\n",
    "            main_live.execute(\"UPDATE variable set value='0' where name='live'\");\n",
    "            db_live.commit()\n",
    "            in_upload= False\n",
    "            clientsocket.close()\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        main_live.execute(\"select value from variable where name='live'\")\n",
    "        ll = main_live.fetchone()\n",
    "        db_live.commit()\n",
    "        if ll[0]=='1':\n",
    "            in_upload=True\n",
    "            past2 = datetime.datetime.now()\n",
    "            clientsocket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "            clientsocket.connect(('localhost',8089))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam1 = cv2.VideoCapture(\"http://192.168.225.29:8080/video\")\n",
    "# cam1 = cv2.VideoCapture(0)\n",
    "sess = tf.Session(graph=detection_graph)\n",
    "mpast =datetime.datetime.now()\n",
    "\n",
    "for i in range(0,50):\n",
    "    try:\n",
    "        #clean_process()\n",
    "        _,img = cam1.read()\n",
    "        img2 =img.copy()\n",
    "        now = datetime.datetime.now()\n",
    "        if ((now-mpast).total_seconds()) > 0.25:\n",
    "            mpast=now\n",
    "            th1 = myThread1(img2)\n",
    "            th1.start()\n",
    "            \n",
    "        go_live(img)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "cam1.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
