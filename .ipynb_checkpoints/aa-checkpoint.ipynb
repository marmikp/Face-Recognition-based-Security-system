{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy = 1.0, SVM accuracy = 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import h5py\n",
    "import face1 as face1\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from sklearn.manifold import TSNE\n",
    "import threading\n",
    "import os\n",
    "import time\n",
    "import six.moves.urllib as urllib\n",
    "import tarfile\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import cv2\n",
    "from handle import *\n",
    "sys.path.append(\"..\")\n",
    "import warnings\n",
    "# Suppress LabelEncoder warning\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import label_map_util\n",
    "\n",
    "from utils import visualization_utils as vis_util\n",
    "\n",
    "#from db_conn import *\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Do You Want Train Data? (Y/N) : \")\n",
    "# ans_user = input()\n",
    "# if ans_user==\"y\" or ans_user == \"Y\":\n",
    "#     face1.create_embedded()\n",
    "#     print(\"Data are Trained...\")\n",
    "\n",
    "\n",
    "\n",
    "#############\n",
    "\n",
    "filename = 'tempFiles/embedded1.hdf5'\n",
    "f = h5py.File(filename, 'r')\n",
    "dic = {}\n",
    "for raw in f:\n",
    "    for j in f[raw]:\n",
    "        dic[str(raw),int(j)] = f[raw][j][:] \n",
    "        \n",
    "f.close()\n",
    "\n",
    "filename = 'tempFiles/embedded2.hdf5'\n",
    "f = h5py.File(filename, 'r')\n",
    "mic = {}\n",
    "for raw in f:\n",
    "    for j in f[raw]:\n",
    "        mic[str(raw),int(j)] = f[raw][j][:] \n",
    "        \n",
    "f.close()\n",
    "    \n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "distances = [] # squared L2 distance between pairs\n",
    "identical = [] # 1 if same identity, 0 otherwise\n",
    "metadata = face1.metadata\n",
    "num = len(metadata)\n",
    "embedded = np.empty(num)\n",
    "embedded2 = np.empty(num)\n",
    "\n",
    "embedded = np.load(\"tempFiles/embedding1.npy\")\n",
    "embedded2 = np.load(\"tempFiles/embedding2.npy\")\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "\n",
    "targets = np.array([m.name for m in metadata])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "# 50 train examples of 10 identities (5 examples each)\n",
    "X_train = embedded[train_idx]\n",
    "# 50 test examples of 10 identities (5 examples each)\n",
    "X_test = embedded[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "\n",
    "svc = svm.SVC(kernel='rbf', gamma=1.0, C=1.0)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, knn.predict(X_test))\n",
    "acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "# print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')\n",
    "\n",
    "\n",
    "\n",
    "targets2 = np.array([m.name for m in metadata])\n",
    "\n",
    "encoder1 = LabelEncoder()\n",
    "encoder1.fit(targets2)\n",
    "\n",
    "y = encoder1.transform(targets2)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "# 50 train examples of 10 identities (5 examples each)\n",
    "X_train = embedded2[train_idx]\n",
    "# 50 test examples of 10 identities (5 examples each)\n",
    "X_test = embedded2[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors=2, metric='euclidean')\n",
    "\n",
    "svc1 = svm.SVC(kernel='rbf', gamma=1, C=1.0, probability=True)\n",
    "\n",
    "knn1.fit(X_train, y_train)\n",
    "svc1.fit(X_train, y_train)\n",
    "\n",
    "acc_knn1 = accuracy_score(y_test, knn1.predict(X_test))\n",
    "acc_svc1 = accuracy_score(y_test, svc1.predict(X_test))\n",
    "print(f'KNN accuracy = {acc_knn1}, SVM accuracy = {acc_svc1}')\n",
    "\n",
    "\n",
    "##############\n",
    "past_time = None\n",
    "def handle_forask():\n",
    "        now = datetime.datetime.now()\n",
    "        global past_time\n",
    "        if past_time == None:\n",
    "                past_time = now\n",
    "        time_diff =int((now-past_time).total_seconds())/60\n",
    "        if time_diff > 15:\n",
    "                past_time = now\n",
    "                th.main_c.execute(\"select * from add_person\")\n",
    "                data = th.main_c.fetchall()\n",
    "                th.main_c.execute(\"truncate table add_person\")\n",
    "                for d in data:\n",
    "                        th.local_c.execute('insert into user_detail(name,no_img,last_update_date) values(%s,%s,%s)',(d[2],1,now.strftime(\"%Y-%m-%d\")))\n",
    "                        os.system(\"mkdir images/\"+str(d[2]))\n",
    "                        os.system(\"mv temp_image/\"+str(d[1])+\" images/\"+str(d[2])+\"/1.jpg\")\n",
    "\n",
    "                th.main_c.execute(\"select * from for_asking where flag = 1\")\n",
    "                data1 = th.main_c.fetchall()\n",
    "                th.main_c.execute(\"delete from for_asking where flag=1\")\n",
    "                for d1 in data1:\n",
    "                        th.local_c.execute(\"select no_img from user_detail where name = %s\",(d1[1]))\n",
    "                        no_img = th.local_c.fetchone()\n",
    "                        no = no_img[0] + 1\n",
    "                        sf = len(os.listdir(os.path.join(path, d1[1])))\n",
    "                        s =int(sf)+1\n",
    "                        th.local_c.execute(\"update user_detail set no_img = %s\",(no))\n",
    "                        os.system(\"mv temp_image/\"+str(d1[2])+\" images/\"+str(d1[1])+\"/\"+s+\".jpg\")\n",
    "                #retrain model\n",
    "                \n",
    "\n",
    "\n",
    "################\n",
    "\n",
    "\n",
    "def return_emd(img44):\n",
    "    im = face1.align_image(img44)\n",
    "    # scale RGB values to interval [0,1]\n",
    "    im = (im / 255.).astype(np.float32)\n",
    "    # obtain embedding vector for image\n",
    "    myemd = face1.nn4_small2_pretrained.predict(np.expand_dims(im, axis=0))[0]\n",
    "    \n",
    "    return myemd\n",
    "\n",
    "\n",
    "###############\n",
    "\n",
    "def testing_run(frame):\n",
    "    \n",
    "    #try:\n",
    "        img= frame.copy()\n",
    "        face_locations = face_recognition.face_locations(img)\n",
    "        \n",
    "\n",
    "        #print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "        if len(face_locations) ==0 :\n",
    "            print(\"noface\")\n",
    "            #data_maintain(\"noface\",img)\n",
    "        else:    \n",
    "            for face_location in face_locations:\n",
    "\n",
    "                # Print the location of each face in this image\n",
    "                top, right, bottom, left = face_location\n",
    "                # You can access the actual face itself like this:\n",
    "                face_image = img[top:bottom, left:right]\n",
    "                pil_image = Image.fromarray(face_image)\n",
    "                \n",
    "                \n",
    "            \n",
    "                example_image = np.asarray(face_image)\n",
    "\n",
    "                fd = face1.face_recognition.face_encodings(example_image)[0]\n",
    "                example_prediction1 = knn1.predict([fd])\n",
    "\n",
    "                example_identity1 = encoder1.inverse_transform(example_prediction1)[0]\n",
    "                thresh =0.35\n",
    "                thresh2 =0.40\n",
    "                \n",
    "                if face1.distance(mic[(example_identity1,0)],fd) < thresh and face1.distance(mic[(example_identity1,1)],fd) < thresh :\n",
    "                    print(example_identity1)\n",
    "                    #print(\"1\")\n",
    "                    return example_identity1\n",
    "                else:\n",
    "                    print(\"unknown\")\n",
    "                    return \"unknown\"\n",
    "            \n",
    "   \n",
    "    # except Exception as e:\n",
    "    #     print(e)\n",
    "\n",
    "###################################\n",
    "\n",
    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 90\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "  file_name = os.path.basename(file.name)\n",
    "  if 'frozen_inference_graph.pb' in file_name:\n",
    "    tar_file.extract(file, os.getcwd())\n",
    "\n",
    "# ## Load a (frozen) Tensorflow model into memory.\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class myThread1(threading.Thread):\n",
    "    def __init__(self, image, per):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.time = time\n",
    "        self.image = image\n",
    "        self.per = per\n",
    "        self.db = PyMySQL.connect(\"localhost\",\"root\",\"\",\"main_db\" )\n",
    "        self.db1=PyMySQL.conenct(\"localhost\",\"root\",\"\",\"local_db\" )\n",
    "        self.main_c = self.db.cursor()\n",
    "        self.local_c=self.db1.cursor()\n",
    " \n",
    "        \n",
    "    def run(self):\n",
    "        testing_run(self.image)\n",
    "        self.db.commit()\n",
    "        self.db1.commit()\n",
    "        self.main_c.close()\n",
    "        self.local_c.close()\n",
    "        self.db.close()\n",
    "        self.db1.close()\n",
    "\n",
    "        \n",
    "         \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detection(img):\n",
    "    image_np = img\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    #print(type(image_np_expanded))\n",
    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "    scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "    (boxes, scores, classes, num_detections) = sess.run([boxes, scores, classes, num_detections],feed_dict={image_tensor: image_np_expanded})\n",
    "    image_np,name1 = vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        np.squeeze(boxes),\n",
    "        np.squeeze(classes).astype(np.int32),\n",
    "        np.squeeze(scores),\n",
    "        category_index,\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=8)\n",
    "    return name1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaimin\n",
      "jaimin\n",
      "jaimin\n",
      "jaimin\n",
      "list index out of range\n",
      "jaimin\n",
      "jaimin\n",
      "jaimin\n",
      "noface\n",
      "noface\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Already closed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-540f4af47627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mcam1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;34m\"\"\"Send the quit message and close the socket\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Already closed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Already closed"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cam1 = cv2.VideoCapture(0)\n",
    "sess = tf.Session(graph=detection_graph)\n",
    "for i in range(10):\n",
    "    try:\n",
    "        _,img = cam1.read()\n",
    "        name =testing_run(img)\n",
    "        # if name != \"unknown\":\n",
    "        #     cur.execute(\"update main set lk=1 where id=1\")\n",
    "        #     db.commit()\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(img, str(name), (25, 25), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"img\",img)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        # press 'q' to exit\n",
    "        if k == ord('q'):\n",
    "            break\n",
    "        # per = detection(img2.copy())\n",
    "        # th1 = myThread1(img2,per)\n",
    "        # th1.start()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cam1.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
